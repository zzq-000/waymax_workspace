{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import jaxlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mediapy\n",
    "from tqdm import tqdm\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "from waymax import config as _config\n",
    "from waymax import dataloader\n",
    "from waymax.config import DatasetConfig, DataFormat\n",
    "from waymax import datatypes\n",
    "from waymax import dynamics\n",
    "from waymax import env as _env\n",
    "from waymax import agents\n",
    "from waymax import visualization\n",
    "from waymax.dataloader.dataloader_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./v1_1_0_uncompressed_tf_example_validation_validation_tfexample.tfrecord@150\"\n",
    "\n",
    "files_to_load = generate_sharded_filenames(path)\n",
    "# print(type(files_to_load))\n",
    "# for i in files_to_load:\n",
    "#     print(i)\n",
    "files = tf.data.Dataset.from_tensor_slices(files_to_load)\n",
    "# print(files)\n",
    "# print(type(files))\n",
    "# for file in files:\n",
    "#     print(type(file))\n",
    "# files = tf.data.Dataset.from_tensor_slices(files_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconfig = DatasetConfig(\n",
    "    # path='./training.tfrecord-00100-of-01000',\n",
    "    # path='gs://waymo_open_dataset_motion_v_1_1_0/uncompressed/tf_example/training/training_tfexample.tfrecord@1000',\n",
    "    # path=\"/home/yqnj/testing/testing.tfrecord@150\",\n",
    "    # path = './training.tfrecord@1',\n",
    "    # path = \"./validation.tfrecord@150\",\n",
    "    # path =\"./v1_1_0_uncompressed_tf_example_training_training_tfexample.tfrecord-00000-of-01000\",\n",
    "    path = \"./v1_1_0_uncompressed_tf_example_validation_validation_tfexample.tfrecord@150\",\n",
    "    max_num_rg_points=20000,\n",
    "    data_format=DataFormat.TFRECORD,\n",
    ")\n",
    "max_num_objects = 8\n",
    "config = dataclasses.replace(myconfig, max_num_objects=max_num_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)\n",
      "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "==========\n",
      "<_ShardDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data = tf.constant([1, 2, 3, 4, 5])\n",
    "print(data)\n",
    "data = tf.data.Dataset.from_tensor_slices(data)\n",
    "print(data)\n",
    "for i in data:\n",
    "    print(i)\n",
    "data = data.shard(2, 1)\n",
    "\n",
    "print(\"=\" * 10)\n",
    "\n",
    "print(data)\n",
    "for i in data:\n",
    "    print(i)\n",
    "print(jax.process_count())\n",
    "print(jax.process_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yqnj/miniconda3/envs/waymax/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
    "             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "def parse_fn(filename):\n",
    "  return tf.data.Dataset.range(10)\n",
    "dataset = dataset.interleave(lambda x:\n",
    "    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
    "    cycle_length=4, block_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'1' b'2' b'3' b'4' b'5'], shape=(5,), dtype=string)\n",
      "tf.Tensor([b'2' b'4' b'6' b'8' b'10'], shape=(5,), dtype=string)\n",
      "tf.Tensor([b'4' b'8' b'16' b'32' b'64'], shape=(5,), dtype=string)\n",
      "tf.Tensor([b'1' b'2' b'3' b'4' b'5'], shape=(5,), dtype=string)\n",
      "tf.Tensor([b'2' b'4' b'6' b'8' b'10'], shape=(5,), dtype=string)\n",
      "tf.Tensor(b'1 2 3 4 5', shape=(), dtype=string)\n",
      "tf.Tensor(b'1 2 3 4 5', shape=(), dtype=string)\n",
      "tf.Tensor(b'1 2 3 4 5', shape=(), dtype=string)\n",
      "==========\n",
      "tf.Tensor(b'1 2 3 4 5', shape=(), dtype=string)\n",
      "tf.Tensor(b'1 2 3 4 5', shape=(), dtype=string)\n",
      "tf.Tensor(b'1 2 3 4 5', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 定义一些文件名\n",
    "filenames = [\"file1.txt\", \"file2.txt\", \"file3.txt\"]\n",
    "\n",
    "# 创建一个数据集，包含这些文件名\n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "\n",
    "# 定义一个解析函数，用于将文本行解析成张量\n",
    "def parse_line(line):\n",
    "    return tf.strings.split(line)  # 这里仅以空格分割文本行\n",
    "\n",
    "# 使用interleave函数来交替处理文件中的文本行\n",
    "# lambda x: tf.data.TextLineDataset(x) 会将文件名转化为对应文件的数据集\n",
    "# map(parse_line) 将解析函数应用于每个数据集\n",
    "interleaved_dataset = dataset.interleave(\n",
    "    lambda x: tf.data.TextLineDataset(x).map(parse_line),\n",
    "    cycle_length=len(filenames),  # 交替处理的文件数\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "\n",
    "# 打印前几个元素\n",
    "for element in interleaved_dataset.take(5):\n",
    "    print(element)\n",
    "\n",
    "dataset = tf.data.TextLineDataset(\"file1.txt\")\n",
    "for i in dataset:\n",
    "    print(i)\n",
    "\n",
    "dataset = dataset.repeat(1)\n",
    "\n",
    "print('=' * 10)\n",
    "for i in dataset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "# NOTE: New lines indicate \"block\" boundaries.\n",
    "dataset = dataset.interleave(\n",
    "    lambda x: tf.data.Dataset.from_tensors(x).repeat(6),\n",
    "    cycle_length=2)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from waymax.dataloader import womd_utils\n",
    "myconfig = DatasetConfig(\n",
    "    path = \"./v1_1_0_uncompressed_tf_example_training_training_tfexample.tfrecord-00000-of-01000\",\n",
    "    max_num_rg_points=20000,\n",
    "    data_format=DataFormat.TFRECORD,\n",
    ")\n",
    "data = tf.data.TFRecordDataset(\"./v1_1_0_uncompressed_tf_example_training_training_tfexample.tfrecord-00000-of-01000\")\n",
    "def preprocess_serialized_womd_data(\n",
    "    serialized: bytes, config: _config.DatasetConfig\n",
    ") -> dict[str, tf.Tensor]:\n",
    "  \"\"\"Parses serialized tf example into tf Tensor dict.\"\"\"\n",
    "  womd_features = womd_utils.get_features_description(\n",
    "      include_sdc_paths=config.include_sdc_paths,\n",
    "      max_num_rg_points=config.max_num_rg_points,\n",
    "      num_paths=config.num_paths,\n",
    "      num_points_per_path=config.num_points_per_path,\n",
    "  )\n",
    "\n",
    "  deserialized = tf.io.parse_example(serialized, womd_features)\n",
    "  return preprocess_womd_example(\n",
    "      deserialized,\n",
    "      aggregate_timesteps=config.aggregate_timesteps,\n",
    "      max_num_objects=config.max_num_objects,\n",
    "  )\n",
    "\n",
    "\n",
    "def preprocess_womd_example(\n",
    "    example: dict[str, tf.Tensor],\n",
    "    aggregate_timesteps: bool,\n",
    "    max_num_objects: Optional[int] = None,\n",
    ") -> dict[str, tf.Tensor]:\n",
    "  \"\"\"Preprocesses dict of tf tensors, keyed by str.\"\"\"\n",
    "\n",
    "  if aggregate_timesteps:\n",
    "    processed = womd_utils.aggregate_time_tensors(example)\n",
    "    wrap_yaws = lambda yaws: (yaws + jnp.pi) % (2 * jnp.pi) - jnp.pi\n",
    "    processed['state/all/bbox_yaw'] = wrap_yaws(processed['state/all/bbox_yaw'])\n",
    "  else:\n",
    "    processed = example\n",
    "\n",
    "  if max_num_objects is not None:\n",
    "    # TODO check sdc included if it is needed.\n",
    "    return {\n",
    "        k: v[:max_num_objects] if k.startswith('state/') else v\n",
    "        for k, v in processed.items()\n",
    "    }\n",
    "  else:\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_serialized_womd_data(data, config=myconfig)\n",
    "\n",
    "preprocess_fn = functools.partial(preprocess_serialized_womd_data, config=myconfig)\n",
    "\n",
    "data = data.map(\n",
    "        preprocess_fn, num_parallel_calls=AUTOTUNE, deterministic=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "# print(data)\n",
    "for i in data:\n",
    "    # print(i)\n",
    "    print(len(i.keys()))\n",
    "    break\n",
    "# print(len(data.key()))\n",
    "\n",
    "# cnt = 0\n",
    "# for i in data:\n",
    "#     print(cnt)\n",
    "#     cnt += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waymax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
